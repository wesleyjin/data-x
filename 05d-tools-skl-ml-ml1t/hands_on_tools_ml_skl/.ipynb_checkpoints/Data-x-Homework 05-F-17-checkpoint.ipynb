{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)\n",
    "\n",
    "\n",
    "## Classification BKHW\n",
    "\n",
    "Author List: \n",
    "\n",
    "Sana Iqbal, significant edits for fall 2017\n",
    "\n",
    "Kevin Li, Ikhlaq Sidhu, Spring 2017\n",
    "\n",
    "Original Sources: http://scikit-learn.org,http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "License: Feel free to do whatever you want to with this code\n",
    "\n",
    "\n",
    "### Our  predictive machine learning models perform two types of tasks:\n",
    "\n",
    "* __CLASSIFICATION__:\n",
    "LABELS ARE DISCRETE VALUES.\n",
    "Here the model is trained to classify each instance into a set of predefined  discrete classes.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a  class of that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __defaulter or non-defaulter__ as labels. When we input income and expenditure data  of any customer in this model, it will predict whether the customer is going to default or not.\n",
    "\n",
    "* __REGRESSION__:\n",
    "LABELS ARE CONTINUOUS VALUES.\n",
    "Here the model is trained to predict a continuous value for each instance.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a continuous value  for  that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __ default amount__ as the label. This model when input with income and expenditure data of any customer will be able to predict the default amount the customer might end up with.\n",
    "\n",
    "\n",
    "* __TO GET STARTED:__:\n",
    "\n",
    "We will use python library -SCIKIT-LEARN for our classification and regression models.\n",
    "\n",
    "1. Install numpy, scipy, scikit-learn.\n",
    "\n",
    "2. Download the dataset provided and save it in your current working directory.\n",
    "\n",
    "3. In the following sections  you will:\n",
    "\n",
    "    3.1 Read the dataset into the python program.\n",
    "    \n",
    "    3.2 Look  into the dataset characteristics, check for feature type - categorical or numerical.\n",
    "    \n",
    "    3.3 Find feature distributions to check sufficiency of data.\n",
    "    \n",
    "    3.4 Divide the dataset into training and validation subsets.\n",
    "    \n",
    "    3.5 Fit models with training data  using scikit-learn library.\n",
    "    \n",
    "    3.6 Calculate training error,this gives you the idea of bias in your model.\n",
    "    \n",
    "    3.7 Test model prediction accuracy using validation data,this gives you bias and variance error in the model.\n",
    "    \n",
    "    3.8 Report model performance on validation data using different metrics.\n",
    "    \n",
    "    3.9 Save the model parameters in a pickle file so that it can be used for test data.\n",
    "    \n",
    "  Also, if our data set is small we will have fewer examples for validation.\n",
    "This will not give us a a good estimatiion of model error.\n",
    "We can use  k-fold crossvalidation in such situations.\n",
    "In k-fold cross-validation, the shuffled training data is partitioned into k disjoint sets and the model is trained on k âˆ’1 sets and validated on the kth set. This process is repeated k times with each set\n",
    "chosen as the validation set once. The cross-validation accuracy is reported as the average accuracy\n",
    "of the k iterations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Break Out Section\n",
    "\n",
    "\n",
    "## Regression and Classification:\n",
    "__Data Source__:\n",
    "Datafile is in the working directory by name: __Energy.csv__\n",
    "\n",
    "The dataset was created by Angeliki Xifara ( Civil/Structural Engineer) and was processed by Athanasios Tsanas, Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK).\n",
    "\n",
    "__Data Description__:\n",
    "\n",
    "The dataset contains eight attributes of a building (or features, denoted by X1...X8) and response being the heating load on the building, y1. \n",
    "\n",
    "* X1\tRelative Compactness \n",
    "* X2\tSurface Area \n",
    "* X3\tWall Area \n",
    "*  X4\tRoof Area \n",
    "*  X5\tOverall Height \n",
    "* X6\tOrientation \n",
    "*  X7\tGlazing Area \n",
    "*  X8\tGlazing Area Distribution \n",
    "*  y1\tHeating Load \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1:Read the data file in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.1.2: Describe data features in terms of type, distribution range and mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3: Plot feature distributions.This step should give you clues about data sufficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION\n",
    "### Q 2.1:  Bucket values of 'y1' i.e 'Heating Load'  in the original dataset into 3 classes: \n",
    "### 0:'Low' ( < 15),  1:'Medium'  (15-30),    2: 'High'  (>30)   \n",
    "### This converts the given dataset  into a classification problem, classes being *low, medium and high*.  \n",
    "### Use this datset for creating a  logistic regression classifiction model for predicting heating load type of a building. Use test-train split ratio of 0.15\n",
    "### Report training and test accuracies and  confusion matrices.\n",
    "\n",
    "\n",
    "HINT: Use pandas.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2: One of the preprocessing steps in Data science is Feature Scaling i.e getting all our data on the same scale by setting same  Min-Max of feature values. This makes training less sensitive to the scale of features . Scaling is important in algorithms that use distance based classification, SVM or K means or involve gradient descent optimization . \n",
    "### If we  Scale features in the range [0,1] it is called unity based normalization. Perform unity based normalization on the above dataset and train the model again, compare model performance in training and validation with your previous model.\n",
    "refer:http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler  \n",
    "more at: https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  REGRESSION\n",
    "### Q3.1: Train a linear regression model on 85 percent of the given dataset, what are the intercept  and coefficient values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Q3.2: Report model performance using 'ROOT MEAN SQUARE' error metric on:  \n",
    "###   1. Data that was used for training(Training error)   \n",
    "###  2. On the 15 percent of unseen data (test error)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q4: Use varying data amounts  from your training data (100,200,300,400,500,all) to train models and report  training error and validation error.Plot error rates vs number of training examples.Do you see any relation.\n",
    "\n",
    "#### Hint: Shuffle data, convert to arrays, use array indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
