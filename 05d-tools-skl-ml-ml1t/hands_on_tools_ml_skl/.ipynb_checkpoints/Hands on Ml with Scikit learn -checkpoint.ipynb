{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)\n",
    "\n",
    "\n",
    "## Classification BKHW\n",
    "\n",
    "Author List: \n",
    "\n",
    "Sana Iqbal, significant edits for fall 2017\n",
    "\n",
    "Kevin Li, Ikhlaq Sidhu, Spring 2017\n",
    "\n",
    "Original Sources: http://scikit-learn.org,http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "License: Feel free to do whatever you want to with this code\n",
    "\n",
    "\n",
    "### Our  predictive machine learning models perform two types of tasks:\n",
    "\n",
    "* __CLASSIFICATION__:\n",
    "LABELS ARE DISCRETE VALUES.\n",
    "Here the model is trained to classify each instance into a set of predefined  discrete classes.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a  class of that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __defaulter or non-defaulter__ as labels. When we input income and expenditure data  of any customer in this model, it will predict whether the customer is going to default or not.\n",
    "\n",
    "* __REGRESSION__:\n",
    "LABELS ARE CONTINUOUS VALUES.\n",
    "Here the model is trained to predict a continuous value for each instance.\n",
    "On inputting a feature vector into the model, the trained model is able to predict a continuous value  for  that instance.\n",
    "\n",
    "Eg: We train our model using income and expenditure data of bank customers using  __ default amount__ as the label. This model when input with income and expenditure data of any customer will be able to predict the default amount the customer might end up with.\n",
    "\n",
    "\n",
    "* __TO GET STARTED:__:\n",
    "\n",
    "We will use python library -SCIKIT-LEARN for our classification and regression models.\n",
    "\n",
    "1. Install numpy, scipy, scikit-learn.\n",
    "\n",
    "2. Download the dataset provided and save it in your current working directory.\n",
    "\n",
    "3. In the following sections  you will:\n",
    "\n",
    "    3.1 Read the dataset into the python program.\n",
    "    \n",
    "    3.2 Look  into the dataset characteristics, check for feature type - categorical or numerical.\n",
    "    \n",
    "    3.3 Find feature distributions to check sufficiency of data.\n",
    "    \n",
    "    3.4 Divide the dataset into training and validation subsets.\n",
    "    \n",
    "    3.5 Fit models with training data  using scikit-learn library.\n",
    "    \n",
    "    3.6 Calculate training error,this gives you the idea of bias in your model.\n",
    "    \n",
    "    3.7 Test model prediction accuracy using validation data,this gives you bias and variance error in the model.\n",
    "    \n",
    "    3.8 Report model performance on validation data using different metrics.\n",
    "    \n",
    "    3.9 Save the model parameters in a pickle file so that it can be used for test data.\n",
    "    \n",
    "  Also, if our data set is small we will have fewer examples for validation.\n",
    "This will not give us a a good estimatiion of model error.\n",
    "We can use  k-fold crossvalidation in such situations.\n",
    "In k-fold cross-validation, the shuffled training data is partitioned into k disjoint sets and the model is trained on k âˆ’1 sets and validated on the kth set. This process is repeated k times with each set\n",
    "chosen as the validation set once. The cross-validation accuracy is reported as the average accuracy\n",
    "of the k iterations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with a little touch up.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Example on some pandas features that we are going to use:\n",
    "# identifying any Null values in our dataframe,\n",
    "# replacing null values,\n",
    "# indexing dataframe,\n",
    "# shuffling\n",
    "\n",
    "'''\n",
    "np.random.seed(0)\n",
    "example_df = pd.DataFrame(np.random.randn(5,3),columns=['A','B','C'])\n",
    "# Making a few cells  NaN (Not a Number) values\n",
    "example_df.iloc[3:,1:3] = np.nan\n",
    "example_df.iloc[2,0] = np.nan\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if value in a cell is NAN \n",
    "example_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate total no of NANs in a column\n",
    "example_df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find total no of NANs in a row\n",
    "example_df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace nan with values\n",
    "example_df0=example_df.fillna(0)\n",
    "example_df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace nan with values, USE inplace =True ,to change dataframe\n",
    "example_df1=example_df.fillna(example_df.mean())\n",
    "example_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how to shuffle dataframe values\n",
    "from sklearn.utils import shuffle\n",
    "example_df_3= shuffle(example_df1).reset_index(drop=True)\n",
    "#(drop= True) does no save the parent dataframe indices\n",
    "example_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bin data values \n",
    "import math\n",
    "bins=[-math.inf,0,math.inf]\n",
    "labels=['negative','postive']\n",
    "example_df_3['label'] = pd.cut(example_df_3['B'], bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_df_3['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform you categorical data into numeric values\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the classes that you encoded\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "example_df_3['label']=le.transform(example_df_3['label'])\n",
    "example_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you want numeric labels to be convertedback the original categories\n",
    "list(le.inverse_transform([1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "example_df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us make our first Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](icon_iris.png)  \n",
    "__Problem Statement__:  \n",
    "If you have information about the size of sepal of a *flower (iris)*, could you predict its species *(Iris setosa, Iris virginica and Iris versicolor)*.\n",
    "\n",
    "__Given Dataset__:  \n",
    "*iris_classification.csv* file that has 150 samples with 2 features each - *Sepal Length, Sepal Width* measured in centimeters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets us load the  data file iris_classification.csv\n",
    "file_path='iris_classification.csv'\n",
    "data=pd.read_csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# SHUFFLE data instances to randomize the distribution of different classes\n",
    "# Check if data has any NAN  values, you can choose to drop NAN \n",
    "# containing rows or replace NAN  values with mean. median,or any assumed value.\n",
    "\n",
    "data= shuffle(data).reset_index(drop=True)\n",
    "print('Number of NaNs in the dataframe:\\n',data.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets us look at the data after shuffling\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us separate  features and labels in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET FEATURES FROM THE DATA\n",
    "X=data.iloc[:,:-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET LABELS FROM THE DATA\n",
    "Y=data['species']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us look into data classes, and the balance of all the classes in our data\n",
    "print (dict(Y.value_counts())) #gives the count of each label in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# We will map our class labels to integers and then use in modeling.\n",
    "# The mapping is:'versicolor': 0, 'virginica': 1,'setosa' :2 \\n\")\n",
    "# Since we have less labels here, we can simpy map the values\n",
    "\n",
    "\n",
    "Y=Y.map({'versicolor': 0, 'virginica': 1,'setosa' :2})\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WE can use scikit learn label encoder also\n",
    "otherY=data['species']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(otherY.unique())\n",
    "Ye=le.transform(otherY)\n",
    "Ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Feature vector shape=\", X.shape)\n",
    "print(\"Class shape=\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get distribution of each feature\n",
    "data.hist(figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.groupby('species').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check feature distribution of each class, to get an overview of feature and class relationshhip, also useful in validating data\n",
    "data.groupby('species').hist(figsize=(5,5),label=[0,1,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE SKLEARN INBUILT FUNCTION TO BUILD A LOGISTIC REGRESSION MODEL  \n",
    "For Details check :\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "\n",
    "In order to check the validity of our trained model, we keep a part of our dataset hidden from the model during training, called  __Validation data__.\n",
    "\n",
    "Validation data labels are predicted using the trained model and compared with the actual labels of the data.This gives us the idea about how well the model can be trusted for its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation set  using sklearn function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n",
    "print ('Number of samples in training data:',len(x_train))\n",
    "print ('Number of samples in validation data:',len(x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Name our logisticregression object\n",
    "logreg_model = linear_model.LogisticRegression()\n",
    "# This object has several methods like - fit, score, predict,predict_proba\n",
    "\n",
    "# use your training data to train the model i.e set its parameters\n",
    "print ('Training a logistic Regression Model..')\n",
    "logreg_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING ACCURACY\n",
    "\n",
    "training_accuracy=logreg_model.score(x_train,y_train)\n",
    "print ('Training Accuracy:',training_accuracy)\n",
    "\n",
    "# OR :\n",
    "\n",
    "\n",
    "# this line below will predict a category for every row in x_train\n",
    "Z = logreg_model.predict(x_train) \n",
    "\n",
    "\n",
    "# Estimate errors in an array called L- To understand how .score method works look at the function below\n",
    "# def find_error(Y,Z):\n",
    "#     '''Y:actual_labels\n",
    "#     Z:predicted_labels'''\n",
    "    \n",
    "#     L = np.arange(len(Y))\n",
    "#     for i,value in enumerate(Y):\n",
    "#         if value == Z[i]: \n",
    "#             L[i] = 0\n",
    "#         else:\n",
    "#             L[i] = 1\n",
    "\n",
    "#     print (\"Y-actual Z-predicted Error \\n\")\n",
    "#     for i,value in enumerate(Y):\n",
    "#         print (value, Z[i], L[i])\n",
    "#     error_rate=np.average(L)\n",
    "\n",
    "#     print (\"\\nThe error rate is \", error_rate)\n",
    "#     print ('\\nThe accuracy of the model is ',1-error_rate )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find_error(y_train,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VALIDATION ACCURACY: \n",
    "# we will find accuracy of the model \n",
    "# using data that was not used for training the model\n",
    "\n",
    "validation_accuracy=logreg_model.score(x_test,y_test)\n",
    "print('Accuracy of the model on unseen validation data: ',validation_accuracy)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = y_test\n",
    "y_pred = logreg_model.predict(x_test)\n",
    "cf=pd.DataFrame(confusion_matrix(y_true, y_pred),columns=['Pred 0',1,2],index=['Act 0',1,2])\n",
    "print ('Confusion matrix of test data is: \\n',cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLOT THE DECISION BOUNDARIES:\n",
    "# 1.create meshgrid of all points between \n",
    "\n",
    "#For that we will create a mesh between [x_min, x_max]x[y_min, y_max].\n",
    "h = 0.02  # step size in the mesh\n",
    "x1_min = X['sepal_length'].min() - .5\n",
    "x1_max =X['sepal_length'].max() + .5\n",
    "x2_min = X['sepal_width'].min() - .5\n",
    "x2_max = X['sepal_width'].max() + .5\n",
    "\n",
    "\n",
    "\n",
    "# print x_min, x_max, y_min, y_max\n",
    "x1r = np.arange(x1_min, x1_max, h)\n",
    "x2r = np.arange(x2_min, x2_max, h)\n",
    "\n",
    "# returns coordinate matrices from coordinate vectors.\n",
    "feature1, feature2 = np.meshgrid(x1r, x2r)\n",
    "# we can now make prediction on these cordinates, which will\n",
    "# be used for plotting the decision boundary in our 2d feature space\n",
    "Z = logreg_model.predict(np.c_[feature1.ravel(), feature2.ravel()])\n",
    "\n",
    "'''\n",
    "# Translates slice objects to concatenation along the second axis.\n",
    "# >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
    "# provides: array([[1, 2, 3, 0, 0, 4, 5, 6]])\n",
    "'''\n",
    "\n",
    "print ('finished Z')\n",
    "\n",
    "# another approach is to make an array Z2 which has all the predicted values in (xr,yr).  \n",
    "# This takes longer\n",
    "Z2 = np.arange(len(x1r)*len(x2r)).reshape(len(x1r),len(x2r))\n",
    "for x2ni in range(len(x2r)):\n",
    "    for x1ni in range(len(x1r)):\n",
    "#         print (xni, yni, logreg_model.predict([[xr[xni],yr[yni]]]))\n",
    "\n",
    "        Z2[x1ni,x2ni] =logreg_model.predict([[x1r[x1ni],x2r[x2ni]]])\n",
    "print ('finished Z2')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try these to better undertand the code above\n",
    "# print (Z.shape)\n",
    "# print (len(x1r), len(x2r))\n",
    "# print (Z2.shape)\n",
    "# Z2[[1,1]] = 2\n",
    "# print (x1ni, x2ni)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(feature1.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(feature1, feature2, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "plt.scatter(X['sepal_length'], X['sepal_width'], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.colorbar()\n",
    "plt.xlim(x1r.min(), x1r.max())\n",
    "plt.ylim(x2r.min(), x2r.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot of decison boundary\n",
    "Z = Z.reshape(feature1.shape)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.pcolormesh(x1r, x2r, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(x_train['sepal_length'], x_train['sepal_width'], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(x1r.min(), x1r.max())\n",
    "plt.ylim(x2r.min(), x2r.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Z = Z.reshape(feature1.shape)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.pcolormesh(x1r, x2r, Z, cmap=plt.cm.Paired)\n",
    "plt.colorbar()\n",
    "label=np.unique(y_test)\n",
    "plt.title('VALIDATION DATA -ACTUAL LABELS')\n",
    "# Plot also the training points\n",
    "plt.scatter(x_test['sepal_length'], x_test['sepal_width'], c=y_test,label=np.unique(y_test), edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Example\n",
    "\n",
    "__Linear regression__ is a predictive modeling technique for predicting a numeric response variable based on features.  \n",
    "\"Linear\" in the name linear regression refers to the fact that this method fits a model where response bears linear relationship with features. (ie Z is proportional to first power of x)\n",
    "\n",
    "__Z = X0 + a(X1) + b(X2) +.... where:__   \n",
    "Z: predicted response  \n",
    "X0: intercept  \n",
    "a,b,..: Coefficients of X1,X2..  \n",
    "\n",
    "If Y is the actual response and Z is the predicted response,    \n",
    "__Y-Z= Residual__  \n",
    "Average Residual defines model performance,residual equal to zero represents a perfect fit model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Source: Scikit learn\n",
    "Code source: Jaques Grobler\n",
    "License: BSD 3 clause'''\n",
    "from sklearn import linear_model\n",
    "\n",
    "example_dff = pd.DataFrame(np.random.randint(0,100,size=(100, 1)),columns=['X'])\n",
    "example_dff['C']=5.1*example_dff['X']\n",
    "# example_dff['C']=5.1*example_dff['X']**2\n",
    "X_reg=example_dff[['X']]\n",
    "\n",
    "Y_reg=example_dff['C']\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_reg, Y_reg)\n",
    "Z_reg=regr.predict(X_reg)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\",np.mean((Z_reg - Y_reg) ** 2))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(X_reg['X'], Y_reg,  color='red')\n",
    "plt.plot(X_reg['X'], Z_reg, color='blue',\n",
    "         linewidth=3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression using data with one feature -X')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
